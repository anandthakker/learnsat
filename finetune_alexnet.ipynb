{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        description\n",
      "label                              \n",
      "0                           No Data\n",
      "1                        Open Water\n",
      "2                Perennial Ice/Snow\n",
      "3             Developed; Open Space\n",
      "4          Developed; Low Intensity\n",
      "5       Developed; Medium Intensity\n",
      "6          Developed High Intensity\n",
      "7      Barren Land (Rock/Sand/Clay)\n",
      "8                  Deciduous Forest\n",
      "9                  Evergreen Forest\n",
      "10                     Mixed Forest\n",
      "11                      Dwarf Scrub\n",
      "12                      Shrub/Scrub\n",
      "13             Grassland/Herbaceous\n",
      "14                 Sedge/Herbaceous\n",
      "15                          Lichens\n",
      "16                             Moss\n",
      "17                      Pasture/Hay\n",
      "18                 Cultivated Crops\n",
      "19                   Woody Wetlands\n",
      "20     Emergent Herbaceous Wetlands\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy\n",
    "import math\n",
    "import caffe\n",
    "\n",
    "from pprint import pprint\n",
    "from pylab import *\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# helper to show images in square\n",
    "# take an array of shape (n, height, width) or (n, height, width, channels)\n",
    "# and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)\n",
    "def vis_square(data, padsize=1, padval=0, canvas=plt):\n",
    "    for im in data:\n",
    "        pass\n",
    "        #im -= im.min()\n",
    "        #im /= im.max()\n",
    "    \n",
    "    # force the number of filters to be square\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "    padding = ((0, n ** 2 - data.shape[0]), (0, padsize), (0, padsize)) + ((0, 0),) * (data.ndim - 3)\n",
    "    data = np.pad(data, padding, mode='constant', constant_values=(padval, padval))\n",
    "    \n",
    "    # tile the filters into an image\n",
    "    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n",
    "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "    if data.ndim == 4:\n",
    "        canvas.imshow(data)\n",
    "    else:\n",
    "        canvas.imshow(data, cmap='gray')\n",
    "        \n",
    "# Grab descriptions of the labels\n",
    "classes = pd.read_csv('temp/class-list.csv', index_col='label', header=0)\n",
    "print(classes)\n",
    "\n",
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "TRAIN_BATCH = 256\n",
    "TEST_BATCH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def define_solver(base_lr, lr_step, gamma, weight_decay, momentum, snapshot_prefix, snapshot_iters, **kwargs):\n",
    "    return \"\"\"# Generated from python.\n",
    "    # Train Set\n",
    "    #\n",
    "    # Training net\n",
    "    train_net: \"temp/auto-train.prototxt\"\n",
    "    # Test net\n",
    "    test_net: \"temp/auto-test.prototxt\"\n",
    "    # These numbers don't matter, because we're running the test net ourselves.\n",
    "    test_iter: 1\n",
    "    test_interval: 100000\n",
    "    # The base learning rate, momentum and the weight decay of the network.\n",
    "    base_lr: %(base_lr)s\n",
    "    momentum: %(momentum)i\n",
    "    weight_decay: %(weight_decay)s\n",
    "    # The learning rate policy\n",
    "    lr_policy: \"step\"\n",
    "    gamma: %(gamma)f\n",
    "    stepsize: %(lr_step)i\n",
    "    display: 500\n",
    "    # The maximum number of iterations\n",
    "    max_iter: 10000\n",
    "    # snapshot intermediate results\n",
    "    snapshot: %(snapshot_iters)i\n",
    "    snapshot_prefix: \"%(snapshot_prefix)s\"\n",
    "    # solver mode: CPU or GPU\n",
    "    solver_mode: GPU\"\"\" % locals()\n",
    "\n",
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "\n",
    "def define_net(lmdb, lmdb_labels, batch_size, mean_file, **kwargs):\n",
    "    with open('finetune/train_val.prototxt') as f:\n",
    "        return f.read() % locals()\n",
    "\n",
    "## Set up the network\n",
    "def create_solver(train_data, train_labels, **kwargs):\n",
    "    with open('temp/autosolver.prototxt', 'w') as f:\n",
    "        f.write(define_solver(**kwargs))\n",
    "\n",
    "    with open('temp/auto-train.prototxt', 'w') as f:\n",
    "        f.write('name: \"SeedNet\"\\n')\n",
    "        f.write(str(define_net(train_data, train_labels, TRAIN_BATCH,\n",
    "                               train_data + '_mean.binaryproto',\n",
    "                              **kwargs)))\n",
    "\n",
    "    with open('temp/auto-test.prototxt', 'w') as f:\n",
    "        f.write('name: \"SeedNet\"\\n')\n",
    "        f.write(str(define_net('temp/learnsat-val-lmdb',\n",
    "                               'temp/classify/val-labels-lmdb',\n",
    "                               TEST_BATCH,\n",
    "                               train_data + '_mean.binaryproto',\n",
    "                              **kwargs)))\n",
    "\n",
    "        \n",
    "    solver = caffe.SGDSolver('temp/autosolver.prototxt')\n",
    "    solver.net.copy_from(os.path.join(os.environ['CAFFE_ROOT'], 'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel'))\n",
    "    solver.test_nets[0].copy_from(os.path.join(os.environ['CAFFE_ROOT'], 'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel'))\n",
    "\n",
    "    \n",
    "    blob = caffe.proto.caffe_pb2.BlobProto()\n",
    "    data = open( train_data + '_mean.binaryproto' , 'rb' ).read()\n",
    "    blob.ParseFromString(data)\n",
    "    arr = np.array( caffe.io.blobproto_to_array(blob) )\n",
    "    mean_image = arr[0]\n",
    "\n",
    "    # input preprocessing: 'data' is the name of the input blob == net.inputs[0]\n",
    "    transformer = caffe.io.Transformer({'data': solver.net.blobs['data'].data.shape})\n",
    "    transformer.set_transpose('data', (2,0,1))\n",
    "    transformer.set_mean('data', mean_image)\n",
    "    \n",
    "    return solver, transformer\n",
    "\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "def run_test(net, iterations):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for test_it in range(iterations):\n",
    "        net.forward()\n",
    "        y_pred.extend(net.blobs['fc8_sat'].data.argmax(1))\n",
    "        y_true.extend(net.blobs['labels'].data.reshape(net.blobs['fc8_sat'].data.shape).argmax(1))\n",
    "    \n",
    "    accuracy_score = metrics.accuracy_score(y_true, y_pred)\n",
    "    precision_score = metrics.precision_score(y_true, y_pred, average='macro')\n",
    "    recall_score = metrics.recall_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    return accuracy_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def run(prefix, **params):\n",
    "    print('Running with params: ' + str(params))\n",
    "    TRAIN_DATA = 'temp/learnsat-subset-' + str(params['train_size']) + '-train-lmdb'\n",
    "    TRAIN_LABELS = 'temp/classify/train-' + str(params['train_size']) + '-labels-lmdb'\n",
    "    TEST_SIZE = 4000\n",
    "    TEST_EPOCH = TEST_SIZE / TEST_BATCH\n",
    "\n",
    "    train_epoch = params['train_size'] / TRAIN_BATCH\n",
    "    \n",
    "    # convert lr_step from iterations to epochs\n",
    "    params['lr_step'] = params['lr_step'] * train_epoch\n",
    "    \n",
    "    niter = train_epoch * params['n_epochs'] + 1\n",
    "\n",
    "    hp = '.'.join(['-'.join((p, str(params[p]))) for p in params])\n",
    "    params['snapshot_prefix'] = prefix + '.snap.' + hp + '.'\n",
    "    params['snapshot_iters'] = niter - 1\n",
    "\n",
    "    solver, transformer = create_solver(TRAIN_DATA,TRAIN_LABELS,**params)\n",
    "\n",
    "    test_interval = train_epoch\n",
    "    ntest = 1 + int(np.ceil(float(niter-1) / test_interval))\n",
    "    train_loss = zeros(niter)\n",
    "    train_acc = zeros(ntest)\n",
    "    train_precision = zeros(ntest)\n",
    "    train_recall = zeros(ntest)\n",
    "    test_acc = zeros(ntest)\n",
    "    test_precision = zeros(ntest)\n",
    "    test_recall = zeros(ntest)\n",
    "    output = zeros((niter, 16, 21))\n",
    "\n",
    "    # the main solver loop\n",
    "    for it in range(niter):\n",
    "        solver.step(1)  # SGD by Caffe\n",
    "\n",
    "        # store the train loss\n",
    "        train_loss[it] = solver.net.blobs['loss'].data\n",
    "\n",
    "        # run a full test every so often\n",
    "        # (Caffe can also do this for us and write to a log, but we show here\n",
    "        #  how to do it directly in Python, where more complicated things are easier.)\n",
    "        if it % test_interval == 0:\n",
    "            index = it // test_interval\n",
    "            test_acc[index], test_precision[index], test_recall[index] = run_test(solver.test_nets[0], TEST_EPOCH)\n",
    "            train_acc[index], train_precision[index], train_recall[index] = run_test(solver.net, train_epoch)\n",
    "    \n",
    "    # save results to file\n",
    "    if prefix:\n",
    "        last_acc = test_acc[-1]\n",
    "        filePrefix = prefix + '.val-' + str(last_acc) + '.' + hp\n",
    "        with open(filePrefix + '.params.json', 'w') as f:\n",
    "            f.write(json.dumps(params))\n",
    "        np.save(filePrefix + '.train_loss.npy', train_loss)\n",
    "        np.save(filePrefix + '.train_acc.npy', train_acc)\n",
    "        np.save(filePrefix + '.val_acc.npy', test_acc)\n",
    "\n",
    "    return (\n",
    "        solver,\n",
    "        transformer,\n",
    "        params,\n",
    "        niter,\n",
    "        train_epoch,\n",
    "        test_interval,\n",
    "        train_loss,\n",
    "        train_acc,\n",
    "        train_precision,\n",
    "        train_recall,\n",
    "        test_acc,\n",
    "        test_precision,\n",
    "        test_recall)\n",
    "\n",
    "\n",
    "def plot_result(\n",
    "        params,\n",
    "        niter,\n",
    "        train_epoch,\n",
    "        test_interval,\n",
    "        train_loss,\n",
    "        train_acc,\n",
    "        train_precision,\n",
    "        train_recall,\n",
    "        test_acc,\n",
    "        test_precision,\n",
    "        test_recall):\n",
    "    \n",
    "    fig, (ax1, bx1) = subplots(1, 2, figsize=(12,5))\n",
    "    fig.suptitle(str(params))\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(arange(niter) / float(train_epoch), train_loss)\n",
    "    ax2.plot(test_interval * arange(len(test_acc)) / train_epoch, test_acc, 'r',\n",
    "             test_interval * arange(len(train_acc)) / train_epoch, train_acc, 'g')\n",
    "    #ax3 = ax2.twinx()\n",
    "    #ax3.plot(test_interval * arange(len(test_acc)), train_acc, 'g')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    ax1.set_ylabel('train loss')\n",
    "    ax1.set_ylim((0,16))\n",
    "    ax2.set_ylabel('accuracy')\n",
    "    ax2.set_ylim((0,1))\n",
    "    #ax3.set_ylabel('test accuracy')\n",
    "\n",
    "    fscore = test_precision * test_recall / (test_precision + test_recall)\n",
    "    bx2 = bx1.twinx()\n",
    "    bx1.plot(arange(niter), train_loss)\n",
    "    bx2.plot(test_interval * arange(len(test_acc)), test_precision, 'r',\n",
    "             test_interval * arange(len(train_acc)), test_recall, 'y',\n",
    "             test_interval * arange(len(train_acc)), fscore, 'g')\n",
    "    #ax3 = ax2.twinx()\n",
    "    #ax3.plot(test_interval * arange(len(test_acc)), train_acc, 'g')\n",
    "    bx1.set_xlabel('iteration')\n",
    "    bx1.set_ylabel('train loss')\n",
    "    bx1.set_ylim((0,None))\n",
    "    bx2.set_ylabel('precision, recall, fscore')\n",
    "    bx2.set_ylim((0,1))\n",
    "    #ax3.set_ylabel('test accuracy')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with params: {'lr_step': 5, 'n_epochs': 1, 'train_size': 10240, 'base_lr': 0.00826074106984656, 'momentum': 0.5, 'weight_decay': 0.04840675221764321, 'gamma': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "rates = [10 ** uniform(-4, -1) for x in range(4)]\n",
    "lambdas = [10 ** uniform(-3, 0) for x in range(4)]\n",
    "params = [\n",
    "    dict(\n",
    "        n_epochs=60,\n",
    "        train_size=10240,\n",
    "        weight_decay=wd,\n",
    "        base_lr=lr,\n",
    "        momentum=mu,\n",
    "        gamma=0.1,\n",
    "        lr_step=lr_step\n",
    "    )\n",
    "    for lr in rates\n",
    "    for lr_step in [5, 10, 20, 40]\n",
    "    for wd in lambdas\n",
    "    for mu in [0.5, 0.9, 0.95, 0.99]\n",
    "]\n",
    "\n",
    "results = [run('finetune/search/alex', **hp)[2:] for hp in params[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'base_lr': 0.004157626033358791,\n",
       "  'gamma': 0.1,\n",
       "  'lr_step': 20,\n",
       "  'momentum': 0.9,\n",
       "  'n_epochs': 5,\n",
       "  'train_size': 4096,\n",
       "  'weight_decay': 0.04069495718604703},\n",
       " {'base_lr': 0.004157626033358791,\n",
       "  'gamma': 0.1,\n",
       "  'lr_step': 20,\n",
       "  'momentum': 0.9,\n",
       "  'n_epochs': 5,\n",
       "  'train_size': 4096,\n",
       "  'weight_decay': 0.025323647758374014},\n",
       " {'base_lr': 0.004157626033358791,\n",
       "  'gamma': 0.1,\n",
       "  'lr_step': 20,\n",
       "  'momentum': 0.9,\n",
       "  'n_epochs': 5,\n",
       "  'train_size': 4096,\n",
       "  'weight_decay': 0.0012418741132024511},\n",
       " {'base_lr': 0.004157626033358791,\n",
       "  'gamma': 0.1,\n",
       "  'lr_step': 20,\n",
       "  'momentum': 0.9,\n",
       "  'n_epochs': 5,\n",
       "  'train_size': 4096,\n",
       "  'weight_decay': 0.001277685015719408},\n",
       " {'base_lr': 0.0074787508611772885,\n",
       "  'gamma': 0.1,\n",
       "  'lr_step': 20,\n",
       "  'momentum': 0.9,\n",
       "  'n_epochs': 5,\n",
       "  'train_size': 4096,\n",
       "  'weight_decay': 0.13255204871387155},\n",
       " {'base_lr': 0.0074787508611772885,\n",
       "  'gamma': 0.1,\n",
       "  'lr_step': 20,\n",
       "  'momentum': 0.9,\n",
       "  'n_epochs': 5,\n",
       "  'train_size': 4096,\n",
       "  'weight_decay': 0.007865003196694803},\n",
       " {'base_lr': 0.0074787508611772885,\n",
       "  'gamma': 0.1,\n",
       "  'lr_step': 20,\n",
       "  'momentum': 0.9,\n",
       "  'n_epochs': 5,\n",
       "  'train_size': 4096,\n",
       "  'weight_decay': 0.0012673916612323032},\n",
       " {'base_lr': 0.0074787508611772885,\n",
       "  'gamma': 0.1,\n",
       "  'lr_step': 20,\n",
       "  'momentum': 0.9,\n",
       "  'n_epochs': 5,\n",
       "  'train_size': 4096,\n",
       "  'weight_decay': 0.3704950310450504},\n",
       " {'base_lr': 0.06562880658583453,\n",
       "  'gamma': 0.1,\n",
       "  'lr_step': 20,\n",
       "  'momentum': 0.9,\n",
       "  'n_epochs': 5,\n",
       "  'train_size': 4096,\n",
       "  'weight_decay': 0.009581567387817997},\n",
       " {'base_lr': 0.06562880658583453,\n",
       "  'gamma': 0.1,\n",
       "  'lr_step': 20,\n",
       "  'momentum': 0.9,\n",
       "  'n_epochs': 5,\n",
       "  'train_size': 4096,\n",
       "  'weight_decay': 0.014105314338305976},\n",
       " {'base_lr': 0.06562880658583453,\n",
       "  'gamma': 0.1,\n",
       "  'lr_step': 20,\n",
       "  'momentum': 0.9,\n",
       "  'n_epochs': 5,\n",
       "  'train_size': 4096,\n",
       "  'weight_decay': 0.6137229753582566},\n",
       " {'base_lr': 0.06562880658583453,\n",
       "  'gamma': 0.1,\n",
       "  'lr_step': 20,\n",
       "  'momentum': 0.9,\n",
       "  'n_epochs': 5,\n",
       "  'train_size': 4096,\n",
       "  'weight_decay': 0.6952821217552054},\n",
       " {'base_lr': 0.0017800457497664922,\n",
       "  'gamma': 0.1,\n",
       "  'lr_step': 20,\n",
       "  'momentum': 0.9,\n",
       "  'n_epochs': 5,\n",
       "  'train_size': 4096,\n",
       "  'weight_decay': 0.01334770413939359},\n",
       " {'base_lr': 0.0017800457497664922,\n",
       "  'gamma': 0.1,\n",
       "  'lr_step': 20,\n",
       "  'momentum': 0.9,\n",
       "  'n_epochs': 5,\n",
       "  'train_size': 4096,\n",
       "  'weight_decay': 0.15977746191747422},\n",
       " {'base_lr': 0.0017800457497664922,\n",
       "  'gamma': 0.1,\n",
       "  'lr_step': 20,\n",
       "  'momentum': 0.9,\n",
       "  'n_epochs': 5,\n",
       "  'train_size': 4096,\n",
       "  'weight_decay': 0.6963465093868196},\n",
       " {'base_lr': 0.0017800457497664922,\n",
       "  'gamma': 0.1,\n",
       "  'lr_step': 20,\n",
       "  'momentum': 0.9,\n",
       "  'n_epochs': 5,\n",
       "  'train_size': 4096,\n",
       "  'weight_decay': 0.0018636551597238822}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_result(*result[2:])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = subplots()\n",
    "losses = [res[4][-1] for res in results]\n",
    "accuracy = [res[8][-1] for res in results]\n",
    "lambs = [res[0]['weight_decay'] for res in results]\n",
    "ax.plot(lambs, accuracy, 'g')\n",
    "ax.set_ylim((0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = classes.index.tolist()\n",
    "descriptions = classes.description.tolist()\n",
    "def report(net, iterations):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for test_it in range(iterations):\n",
    "        net.forward()\n",
    "        y_pred.extend(net.blobs['ip2'].data.argmax(1))\n",
    "        y_true.extend(net.blobs['reshape_labels'].data.argmax(1))\n",
    "    \n",
    "    print(metrics.confusion_matrix(y_true, y_pred))\n",
    "    print(metrics.classification_report(y_true, y_pred, labels, descriptions))\n",
    "\n",
    "report(solver.net, TRAIN_EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks\n",
    "\n",
    "After loading up a solver, run these checks to see if it's set up as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solver = result[0]\n",
    "transform = result[1]\n",
    "\n",
    "solver.net.forward()\n",
    "# To get an idea of the architecture of our net, we can check the dimensions of \n",
    "# the intermediate features (blobs) and parameters (these will also be useful to \n",
    "# refer to when manipulating data later).\n",
    "\n",
    "# each output is (batch size, feature dim, spatial dim)\n",
    "pprint([(k, v.data.shape) for k, v in solver.net.blobs.items()])\n",
    "\n",
    "# just print the weight sizes (not biases)\n",
    "pprint([(k, v[0].data.shape) for k, v in solver.net.params.items()])\n",
    "\n",
    "# now draw the network!\n",
    "!$CAFFE_ROOT/python/draw_net.py --rankdir TB temp/auto-train.prototxt temp/net.png\n",
    "from IPython.display import Image\n",
    "Image(filename='temp/net.png', width=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Baseline accuracy.\n",
    "solver.net.forward()  # train net\n",
    "solver.test_nets[0].forward()  # test net (there can be more than one)\n",
    "print(solver.net.blobs['fc8_sat'].data[0])\n",
    "print(solver.net.blobs['labels'].data[0])\n",
    "print(solver.net.blobs['loss'].data)\n",
    "# run an initial forward pass for the full dataset, and see what kind of results we get.\n",
    "# accuracy should be ~ 1/num-classes\n",
    "print(run_test(solver.net, 10240 / TRAIN_BATCH))\n",
    "print(run_test(solver.test_nets[0], 4000 / TEST_BATCH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are we loading train and test images, and their labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#solver.net.forward()\n",
    "index = 0\n",
    "count = 16\n",
    "images = solver.net.blobs['data'].data[index:index+count]\n",
    "images = numpy.array([transform.deprocess('data', i) for i in images])\n",
    "vis_square(images, 1, 0)\n",
    "labels = solver.net.blobs['labels'].data.reshape(TRAIN_BATCH, 21)[index:index+count]\n",
    "pd.DataFrame(labels, columns=classes['description'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from random import randint\n",
    "\n",
    "def softmax(w, t = 1.0):\n",
    "    e = np.exp(np.array(w) / t)\n",
    "    dist = e / np.sum(e)\n",
    "    return dist\n",
    "\n",
    "def top5(data):\n",
    "    df = pd.DataFrame(data, columns=classes['description'].tolist())\n",
    "    return df.T.sort(ascending=False, columns=[0])[:6].T\n",
    "\n",
    "\n",
    "index = randint(0, solver.test_nets[0].blobs['data'].data.shape[0])\n",
    "N = 1\n",
    "\n",
    "print(index)\n",
    "\n",
    "images = solver.test_nets[0].blobs['data'].data[index:index+N]\n",
    "images = numpy.array([transform.deprocess('data', img) for img in images])\n",
    "_, ax = subplots()\n",
    "vis_square(images, 1, 0, ax)\n",
    "\n",
    "row = np.array(['Actual ' + str(i) for i in range(index, index+N)])\n",
    "labels = solver.test_nets[0].blobs['labels'].data.reshape(TEST_BATCH, 21)[index:index + N]\n",
    "predictions = softmax(solver.test_nets[0].blobs['fc8_sat'].data[index:index + N])\n",
    "\n",
    "display(top5(labels))\n",
    "display(top5(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both train and test nets seem to be loading data, and to have correct labels. Let's take one step of (minibatch) SGD and see what happens.\n",
    "\n",
    "Do we have gradients propagating through our filters? Let's see the updates to the first layer, shown here as a $4 \\times 5$ grid of $5 \\times 5$ filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solver.step(1)\n",
    "print(solver.net.params['conv1'][0].diff[:, 0].shape)\n",
    "vis_square(solver.net.params['conv1'][0].diff[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Filters\n",
    "\n",
    "After some training, we can visualize what the filters of our convnet look like, and what kinds of outputs they produce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "plt.rcParams['figure.figsize'] = (5,5)\n",
    "figure()\n",
    "vis_square(numpy.array([transform.deprocess('data', solver.net.blobs['data'].data[index])]))\n",
    "\n",
    "fig, (ax1, ax2) = subplots(1, 2)\n",
    "filters = solver.net.params['conv1'][0].data\n",
    "vis_square(filters.transpose(0, 2, 3, 1), 1, 0, ax1)\n",
    "images = solver.net.blobs['conv1'].data[index]\n",
    "vis_square(images, 1, 0, ax2)\n",
    "\n",
    "fig, (ax1, ax2) = subplots(1, 2)\n",
    "solver.net.params['conv2'][0].data.shape\n",
    "vis_square(solver.net.params['conv2'][0].data[:48].reshape(48**2, 5, 5), 2, 0, ax1)\n",
    "vis_square(solver.net.blobs['conv2'].data[index], 2, 0, ax2)\n",
    "\n",
    "#fig, (ax1, ax2) = subplots(1, 2)\n",
    "#vis_square(solver.net.params['conv3'][0].data[:40].reshape(40**2, 3, 3), 2, 0, ax1)\n",
    "#vis_square(solver.net.blobs['conv3'].data[index], 1, 0, ax2)\n"
   ]
  }
 ],
 "metadata": {
  "description": "Define, train, and test the classic LeNet with the Python interface.",
  "example_name": "Learning LeNet",
  "include_in_docs": true,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "priority": 2
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
